<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jadon Chu">

<title>BBC News Article Classification: A Comparative Study of Supervised Learning Algorithms</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="BBC_News_Classification_files/libs/clipboard/clipboard.min.js"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/popper.min.js"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="BBC_News_Classification_files/libs/quarto-html/anchor.min.js"></script>
<link href="BBC_News_Classification_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BBC_News_Classification_files/libs/quarto-html/quarto-syntax-highlighting-1fe81d0376b2c50856e68e651e390326.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="BBC_News_Classification_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="BBC_News_Classification_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="BBC_News_Classification_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="-1">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#data-exploration-and-feature-engineering" id="toc-data-exploration-and-feature-engineering" class="nav-link" data-scroll-target="#data-exploration-and-feature-engineering">1. Data Exploration and Feature Engineering</a>
  <ul>
  <li><a href="#data-overview-feature-extraction" id="toc-data-overview-feature-extraction" class="nav-link" data-scroll-target="#data-overview-feature-extraction">1.1 Data Overview &amp; Feature Extraction</a></li>
  <li><a href="#term-frequency-analysis" id="toc-term-frequency-analysis" class="nav-link" data-scroll-target="#term-frequency-analysis">1.2 Term Frequency Analysis</a></li>
  </ul></li>
  <li><a href="#model-development-and-evaluation" id="toc-model-development-and-evaluation" class="nav-link" data-scroll-target="#model-development-and-evaluation">2. Model Development and Evaluation</a>
  <ul>
  <li><a href="#naive-bayes-nb" id="toc-naive-bayes-nb" class="nav-link" data-scroll-target="#naive-bayes-nb">2.1 Naive Bayes (NB)</a></li>
  <li><a href="#k-nearest-neighbours-knn" id="toc-k-nearest-neighbours-knn" class="nav-link" data-scroll-target="#k-nearest-neighbours-knn">2.2 k-Nearest Neighbours (kNN)</a></li>
  <li><a href="#support-vector-machines-svm" id="toc-support-vector-machines-svm" class="nav-link" data-scroll-target="#support-vector-machines-svm">2.3 Support Vector Machines (SVM)</a></li>
  <li><a href="#neural-networks-nn" id="toc-neural-networks-nn" class="nav-link" data-scroll-target="#neural-networks-nn">2.4 Neural Networks (NN)</a></li>
  </ul></li>
  <li><a href="#model-comparison-and-hyperparameter-analysis" id="toc-model-comparison-and-hyperparameter-analysis" class="nav-link" data-scroll-target="#model-comparison-and-hyperparameter-analysis">3. Model Comparison and Hyperparameter Analysis</a>
  <ul>
  <li><a href="#training-set-size-vs.-accuracy" id="toc-training-set-size-vs.-accuracy" class="nav-link" data-scroll-target="#training-set-size-vs.-accuracy">3.1 Training Set Size vs.&nbsp;Accuracy</a>
  <ul>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes">3.1.1 Naive Bayes</a></li>
  <li><a href="#knn" id="toc-knn" class="nav-link" data-scroll-target="#knn">3.1.2 kNN</a></li>
  <li><a href="#svm" id="toc-svm" class="nav-link" data-scroll-target="#svm">3.1.3 SVM</a></li>
  <li><a href="#nn" id="toc-nn" class="nav-link" data-scroll-target="#nn">3.1.4 NN</a></li>
  </ul></li>
  <li><a href="#hyperparameter-impact-via-cross-validation" id="toc-hyperparameter-impact-via-cross-validation" class="nav-link" data-scroll-target="#hyperparameter-impact-via-cross-validation">3.2 Hyperparameter Impact (via Cross-Validation)</a>
  <ul>
  <li><a href="#naive-bayes-1" id="toc-naive-bayes-1" class="nav-link" data-scroll-target="#naive-bayes-1">3.2.1 Naive Bayes</a></li>
  <li><a href="#knn-1" id="toc-knn-1" class="nav-link" data-scroll-target="#knn-1">3.2.2 kNN</a></li>
  <li><a href="#svm-1" id="toc-svm-1" class="nav-link" data-scroll-target="#svm-1">3.2.3 SVM</a></li>
  <li><a href="#nn-1" id="toc-nn-1" class="nav-link" data-scroll-target="#nn-1">3.2.4 NN</a></li>
  </ul></li>
  <li><a href="#fit-optimized-models" id="toc-fit-optimized-models" class="nav-link" data-scroll-target="#fit-optimized-models">3.3 Fit Optimized Models</a>
  <ul>
  <li><a href="#retrain-nb" id="toc-retrain-nb" class="nav-link" data-scroll-target="#retrain-nb">3.3.1 Retrain NB</a></li>
  <li><a href="#retrain-knn" id="toc-retrain-knn" class="nav-link" data-scroll-target="#retrain-knn">3.3.2 Retrain kNN</a></li>
  <li><a href="#retrain-svm" id="toc-retrain-svm" class="nav-link" data-scroll-target="#retrain-svm">3.3.3 Retrain SVM</a></li>
  <li><a href="#retrain-nn" id="toc-retrain-nn" class="nav-link" data-scroll-target="#retrain-nn">3.3.4 Retrain NN</a></li>
  </ul></li>
  <li><a href="#final-evaluation" id="toc-final-evaluation" class="nav-link" data-scroll-target="#final-evaluation">3.4 Final Evaluation</a>
  <ul>
  <li><a href="#final-notes" id="toc-final-notes" class="nav-link" data-scroll-target="#final-notes">Final Notes</a>
  <ul class="collapse">
  <li><a href="#end-of-report" id="toc-end-of-report" class="nav-link" data-scroll-target="#end-of-report"><em>End of Report</em></a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">BBC News Article Classification: A Comparative Study of Supervised Learning Algorithms</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jadon Chu </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="executive-summary" class="level1">
<h1>Executive Summary</h1>
<p>This project investigates and compares four mainstream supervised learning algorithms - Naive Bayes, k-Nearest Neighbours (kNN), Support Vector Machines (SVM), and Neural Networks (NN) - for classifying BBC news articles into “tech” and “entertainment” categories.</p>
<p>Using modern feature extraction and robust evaluation, we demonstrate that high-dimensional text data can be effectively classified with near-perfect accuracy using several approaches, and we analyze the impact of key hyperparameters and training data size on model performance.</p>
</section>
<section id="data-exploration-and-feature-engineering" class="level1">
<h1>1. Data Exploration and Feature Engineering</h1>
<section id="data-overview-feature-extraction" class="level2">
<h2 class="anchored" data-anchor-id="data-overview-feature-extraction">1.1 Data Overview &amp; Feature Extraction</h2>
<div id="1dc8315b" class="cell" data-execution_count="38">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer, CountVectorizer</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># df info</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head(), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) <span class="co"># glimpse first few rows</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.shape, <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) <span class="co"># 1000 rows, 3 columns</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.info(), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) <span class="co"># no missing values </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df[<span class="st">'Category'</span>].value_counts()) <span class="co"># even distribution of categories</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   ArticleId                                               Text       Category
0       1976  lifestyle governs mobile choice faster better ...           tech
1       1797  french honour director parker british film dir...  entertainment
2       1866  fockers fuel festive film chart comedy meet fo...  entertainment
3       1153  housewives lift channel 4 ratings debut us tel...  entertainment
4        342  u2 desire number one u2 three prestigious gram...  entertainment 

(428, 3) 

&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 428 entries, 0 to 427
Data columns (total 3 columns):
 #   Column     Non-Null Count  Dtype 
---  ------     --------------  ----- 
 0   ArticleId  428 non-null    int64 
 1   Text       428 non-null    object
 2   Category   428 non-null    object
dtypes: int64(1), object(2)
memory usage: 10.2+ KB
None 

Category
tech             216
entertainment    212
Name: count, dtype: int64</code></pre>
</div>
</div>
<div id="b0ba6db2" class="cell" data-execution_count="39">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TF-IDF Vectorization</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_tfidf <span class="op">=</span> tfidf_vectorizer.fit_transform(df[<span class="st">'Text'</span>]) <span class="co"># USE THIS METHOD FOR MODEL BUILDING</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prep test data</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>X_test_tfidf  <span class="op">=</span> tfidf_vectorizer.transform(df_test[<span class="st">'Text'</span>])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print num articles and features</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of articles:"</span>, X_tfidf.shape[<span class="dv">0</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of features (unique words):"</span>, X_tfidf.shape[<span class="dv">1</span>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Show features for 5 articles</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> tfidf_vectorizer.get_feature_names_out()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>sample_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    X_tfidf[:<span class="dv">5</span>].toarray(),</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>feature_names[:]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>sample_df[<span class="st">'ArticleId'</span>] <span class="op">=</span> df[<span class="st">'ArticleId'</span>][:<span class="dv">5</span>].values</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>sample_df[<span class="st">'Category'</span>] <span class="op">=</span> df[<span class="st">'Category'</span>][:<span class="dv">5</span>].values</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Sample of 5 articles with TF-IDF features (first 10 shown):</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_df)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Number of articles: 428
Number of features (unique words): 13518

Sample of 5 articles with TF-IDF features (first 10 shown):

    00       000  000th  001st  0051  007  0100  0130  028   05  ...  zombies  \
0  0.0  0.020115    0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0  ...      0.0   
1  0.0  0.000000    0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0  ...      0.0   
2  0.0  0.000000    0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0  ...      0.0   
3  0.0  0.000000    0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0  ...      0.0   
4  0.0  0.000000    0.0    0.0   0.0  0.0   0.0   0.0  0.0  0.0  ...      0.0   

   zone  zonealarm  zones  zoom  zooms   zooropa  zorro  ArticleId  \
0   0.0        0.0    0.0   0.0    0.0  0.000000    0.0       1976   
1   0.0        0.0    0.0   0.0    0.0  0.000000    0.0       1797   
2   0.0        0.0    0.0   0.0    0.0  0.000000    0.0       1866   
3   0.0        0.0    0.0   0.0    0.0  0.000000    0.0       1153   
4   0.0        0.0    0.0   0.0    0.0  0.054551    0.0        342   

        Category  
0           tech  
1  entertainment  
2  entertainment  
3  entertainment  
4  entertainment  

[5 rows x 13520 columns]</code></pre>
</div>
</div>
<p>We use <code>TfidfVectorizer</code> to convert the news articles into numerical feature vectors based on the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. TF-IDF computes a value for each word in each article based on how frequently the word appears in that article (term frequency) and how rare it is across the entire dataset (inverse document frequency). This helps highlight words that are important for distinguishing one article from another, while down-weighting common words that appear across most documents.</p>
<p>We choose TF-IDF for model building because it generally improves the performance of classification algorithms like Naive Bayes, kNN, SVM, and Neural Networks by emphasizing informative, distinguishing words. We used TF-IDF in task 1a so that our feature vectors match what will be used by our classifiers.</p>
<p>However, in next part (1.2 Term Frequency Analysis), we use <code>CountVectorizer</code> (which gives simple word occurrence counts) for term frequency analysis and plots. This is because term frequency plots are most meaningful when they show raw counts of how often each word appears, which is standard for exploratory data analysis of text.</p>
<p>Note that news bodies were lower-cased with removal of stop words and special characters, so we do not have to do any further data preprocessing.</p>
<p>Note that for all model-building below, the same preprocessing method is applied so the data is always consistent.</p>
</section>
<section id="term-frequency-analysis" class="level2">
<h2 class="anchored" data-anchor-id="term-frequency-analysis">1.2 Term Frequency Analysis</h2>
<p>We examine the raw occurrence counts of words in the dataset, helping to identify the most common terms and their distribution across different article categories.</p>
<div id="770abb09" class="cell" data-execution_count="40">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Raw counts</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>count_vectorizer <span class="op">=</span> CountVectorizer()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>X_counts <span class="op">=</span> count_vectorizer.fit_transform(df[<span class="st">'Text'</span>])</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>word_counts <span class="op">=</span> X_counts.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>).A1</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>terms <span class="op">=</span> count_vectorizer.get_feature_names_out()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">### 1. Top-50 term distribution</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>top_50_indices <span class="op">=</span> word_counts.argsort()[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">50</span>]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>top_50_terms <span class="op">=</span> terms[top_50_indices]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">#print(top_50_terms)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>top_50_counts <span class="op">=</span> word_counts[top_50_indices]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">#print(top_50_counts)</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.bar(top_50_terms, top_50_counts)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"(i) Top-50 Term Frequencies in all Articles"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Term"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Frequency"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co">### 2. Term frequency per class</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cat <span class="kw">in</span> df[<span class="st">'Category'</span>].unique():</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> (df[<span class="st">'Category'</span>] <span class="op">==</span> cat).values</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    class_counts <span class="op">=</span> X_counts[mask].<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>).A1</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    top_50_indices <span class="op">=</span> class_counts.argsort()[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">50</span>]</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    plt.bar(terms[top_50_indices], class_counts[top_50_indices])</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"(ii) Top-50 Term Frequencies for Class: </span><span class="sc">{</span>cat<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Term"</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Frequency"</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co">### iii) Class distribution</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> df[<span class="st">'Category'</span>].value_counts()</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> class_counts.plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'(iii) Class Distribution'</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Number of Articles'</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Category'</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co"># text labels</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(class_counts):</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    plt.text(</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        i, <span class="co"># bar center</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        v <span class="op">-</span> <span class="dv">10</span>, <span class="co"># height </span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">str</span>(v), <span class="co"># text </span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        ha<span class="op">=</span><span class="st">'center'</span>, <span class="co"># horiz aligned center</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        va<span class="op">=</span><span class="st">'top'</span>, <span class="co"># vert aligned top</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">11</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-4-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-4-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Top-50 Term Frequencies in all Articles</strong></p>
<p>The first plot shows the 50 most frequent terms across the entire BBC news dataset. Words like “said”, “people”, and “new” appear at the top, reflecting common themes and reporting styles in the news articles. “Said” is by far the most frequent word, likely because news articles often quote sources or interviewees. While some domain-specific words (such as “music”, “film”, or “mobile”) are present, many frequent words are common reporting words used in both tech and entertainment articles.</p>
<p><strong>Top-50 Term Frequencies by Class</strong> - The top terms in tech articles include “mobile”, “technology”, “users”, “software”, “games”, “phone” etc. which are strongly indicative of technology topics. This suggests that the dataset is well-separated.</p>
<ul>
<li>In contrast, the most frequent words in entertainment articles are “film”, “music”, “show”, “awards”, “actor”, “band” etc. which clearly reflects the topics covered in this class.</li>
</ul>
<p>While there is some overlap in common words (such as “said”, “year”, and “tv”), the most distinguishing terms for each class clearly align with their respective domains e.g.&nbsp;“mobile” and “technology” are prominent in tech while “film” and “music” dominate entertainment.</p>
<p><strong>Class distribution</strong></p>
<p>The final plot shows the distribution of articles by class. The dataset is almost perfectly balanced, with 216 tech articles and 212 entertainment articles. This balanced distribution is advantageous for supervised machine learning, as it will help models learn both classes equally well and prevent bias towards either category.</p>
</section>
</section>
<section id="model-development-and-evaluation" class="level1">
<h1>2. Model Development and Evaluation</h1>
<p><em>In this section, we build and analyze four supervised learning models—Naive Bayes, k-Nearest Neighbours, Support Vector Machines, and Neural Networks—for classifying BBC news articles. For each model, we describe the implementation, highlight key parameters, and visualize how they learn to separate tech and entertainment topics. We also discuss the strengths and limitations of each approach based on their behavior and results</em></p>
<section id="naive-bayes-nb" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-nb">2.1 Naive Bayes (NB)</h2>
<p>We use NB to discover the top 20 likely words per class and the top 20 discriminative words per class, and compare the difference between the two.</p>
<p>Naive Bayes is a probabilistic classifier based on Bayes’ Theorem, which models the conditional probability of a class given observed features. For a feature vector <span class="math inline">\(\mathbf{x}\)</span> and class <span class="math inline">\(y\)</span>, the classifier computes:</p>
<p><span class="math display">\[ P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})} \]</span></p>
<p>Assuming feature independence (the “naive” assumption), <span class="math inline">\(P(\mathbf{x}|y)\)</span> is factorized as <span class="math inline">\(\prod_j P(x_j|y)\)</span>. The predicted class is the one with the highest posterior probability:</p>
<p><span class="math display">\[ \hat{y} = \arg\max_y P(y|\mathbf{x}) = \arg\max_y P(y)\prod_j P(x_j|y) \]</span></p>
<p>This approach is efficient for high-dimensional data like text, where features are often word occurrences, and works well for tasks such as spam filtering and document classification.</p>
<div id="da13eced" class="cell" data-execution_count="41">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Naive Bayes model (multinomial)</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> MultinomialNB()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>nb.fit(X_tfidf, df[<span class="st">'Category'</span>])</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">### Top 20 Most Identifiable Words per Class</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature names and class labels</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> tfidf_vectorizer.get_feature_names_out()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>class_labels <span class="op">=</span> nb.classes_</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>top_words <span class="op">=</span> {}</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Get top 20 words for class i, largest log-prob</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, class_label <span class="kw">in</span> <span class="bu">enumerate</span>(class_labels):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    top20_idx <span class="op">=</span> np.argsort(nb.feature_log_prob_[i])[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">20</span>]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    top_words[class_label] <span class="op">=</span> feature_names[top20_idx]</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Display</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cls <span class="kw">in</span> class_labels:</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top 20 words most likely to occur in class '</span><span class="sc">{</span>cls<span class="sc">}</span><span class="ss">' articles:"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(top_words[cls],<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"___"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co">### Top 20 words maximizing the Discriminative Ratio</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>top_ratio_words <span class="op">=</span> {}</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, class_label <span class="kw">in</span> <span class="bu">enumerate</span>(class_labels):</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log ratio vs all other classes (as only binary, just the other class)</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    log_prob_A <span class="op">=</span> nb.feature_log_prob_[i]</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    log_prob_B <span class="op">=</span> nb.feature_log_prob_[<span class="dv">1</span><span class="op">-</span>i]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    log_ratio <span class="op">=</span> log_prob_A <span class="op">-</span> log_prob_B  <span class="co"># log(A/B) = logA - logB</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    top20_ratio_idx <span class="op">=</span> np.argsort(log_ratio)[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">20</span>]</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    top_ratio_words[class_label] <span class="op">=</span> feature_names[top20_ratio_idx]</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Display:</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cls <span class="kw">in</span> class_labels:</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Top 20 discriminative words for class '</span><span class="sc">{</span>cls<span class="sc">}</span><span class="ss">' (NB log-ratio):"</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(top_ratio_words[cls])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 20 words most likely to occur in class 'entertainment' articles:
['film' 'best' 'said' 'show' 'band' 'music' 'year' 'awards' 'us' 'award'
 'actor' 'album' 'star' 'chart' 'tv' 'also' 'number' 'oscar' 'top' 'new'] 


Top 20 words most likely to occur in class 'tech' articles:
['said' 'people' 'mobile' 'software' 'games' 'phone' 'net' 'users'
 'technology' 'mr' 'microsoft' 'virus' 'computer' 'broadband' 'new' 'use'
 'could' 'would' 'digital' 'game'] 

___

Top 20 discriminative words for class 'entertainment' (NB log-ratio):
['film' 'band' 'best' 'actor' 'album' 'chart' 'oscar' 'singer' 'award'
 'actress' 'star' 'musical' 'stars' 'festival' 'comedy' 'awards' 'aviator'
 'theatre' 'rock' 'nominated']

Top 20 discriminative words for class 'tech' (NB log-ratio):
['mobile' 'software' 'users' 'microsoft' 'games' 'net' 'technology'
 'virus' 'phone' 'broadband' 'computer' 'phones' 'spam' 'mail' 'firms'
 'use' 'spyware' 'online' 'pc' 'internet']</code></pre>
</div>
</div>
<p>Here, we trained a Naive Bayes (multinomial) classifier on the TF-IDF feature representation of the BBC news articles to classify them as either ‘tech’ or ‘entertainment’.</p>
<p>The first two lists display the 20 words with the highest probability of occurring in each class using the equation <span class="math inline">\(P(X_w = 1 \mid Y = y)\)</span>, and the last two lists display the top 20 words that maximize the discriminative ratio between the two classes: <span class="math inline">\(\frac{P(X_w = 1 \mid Y = y)}{P(X_w = 1 \mid Y \neq y)}\)</span>.</p>
<p>This equation in particular, identifies words that are much more likely to appear in one class than the other. In practice we use the log of this ratio for stability such that, the larger the log-ratio, the more ‘distinctive’ the word is for that class.</p>
<p><strong>Which set of words describes the two classes better?</strong></p>
<p>The first set for each class (most likely words) contains terms that are highly frequent in that class, but also includes generic reporting terms such as “said” and “new” which are not unique to the topic e.g.&nbsp;the word “said” appears as a top word for both classes, even though it is not specific to tech or entertainment.</p>
<p>The second set (discriminative log-ratio words) for each class, intead focuses on words that are especially distinctive of each topic. - For entertainment, these include domain-specific words such as “film”, “band”, “actor”, “album”, “oscar” etc. which are strongly associated with movies and music. - For tech, words such as “mobile”, “software”, “users”, “microsoft”, “games” etc. clearly relate to technology topics.</p>
<p>Therefore, the discriminative ratio set better describes and distinguishes each class. These words would be more useful both for human interpretation of class topics and for building classifiers that spearate the two classes.</p>
</section>
<section id="k-nearest-neighbours-knn" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbours-knn">2.2 k-Nearest Neighbours (kNN)</h2>
<p>k-Nearest Neighbours (kNN) is a non-parametric, instance-based learning algorithm that classifies a new data point by comparing it to the <span class="math inline">\(k\)</span> closest points in the training set. Given a query point <span class="math inline">\(\mathbf{x}'\)</span>, kNN computes the distance (commonly Euclidean or Manhattan) to all training instances, selects the <span class="math inline">\(k\)</span> nearest neighbors, and assigns the most frequent class among them:</p>
<p><span class="math display">\[ \hat{y} = \text{majority vote}\left({y_i : \mathbf{x}_i \in \text{NN}_k(\mathbf{x}')}\right) \]</span></p>
<p>where <span class="math inline">\(\text{NN}_k(\mathbf{x}')\)</span> denotes the set of <span class="math inline">\(k\)</span> training points closest to <span class="math inline">\(\mathbf{x}'\)</span>. The choice of <span class="math inline">\(k\)</span> controls the bias-variance tradeoff: small <span class="math inline">\(k\)</span> can lead to overfitting and sensitivity to noise, while large <span class="math inline">\(k\)</span> may cause underfitting. Decision boundaries are determined locally and can take arbitrary shapes, making kNN flexible for complex datasets. Scaling and dimensionality reduction (e.g., PCA) are often used to improve distance calculations in high-dimensional spaces.</p>
<div id="f707a63f" class="cell" data-execution_count="42">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score, make_scorer</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, StratifiedKFold</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Dimensional reduction</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>svd <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>X_train2 <span class="op">=</span> svd.fit_transform(X_tfidf)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>X_test2 <span class="op">=</span> svd.transform(X_test_tfidf)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Label encoded to number</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> {<span class="st">'entertainment'</span>:<span class="dv">0</span>, <span class="st">'tech'</span>:<span class="dv">1</span>}</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>y_train_enc <span class="op">=</span> df[<span class="st">'Category'</span>].<span class="bu">map</span>(encoder)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>y_test_enc  <span class="op">=</span> df_test [<span class="st">'Category'</span>].<span class="bu">map</span>(encoder)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>f1_score_list <span class="op">=</span> []</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>f1_score_manhattan <span class="op">=</span> []</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">428</span>,<span class="dv">2</span>))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"># return f1 score of KNN on euclidean distance metric given train and evaluate data</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_f1(x_train, y_train, x_eval, y_eval, k):</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> KNeighborsClassifier(</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        n_neighbors<span class="op">=</span>k,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        metric<span class="op">=</span><span class="st">'euclidean'</span>,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    model.fit(x_train, y_train)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_eval)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f1_score(y_eval, y_pred,</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                    average<span class="op">=</span><span class="st">'weighted'</span>,</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>                    zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Go through every odd k and save its f1 score</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> ks:</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> knn_f1(X_tfidf, y_train_enc, X_test_tfidf, y_test_enc, k)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    f1_score_list.append(score)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Go through every odd k and save its f1 score on manhattan distance metric.</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> ks:</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    model_manhattan <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k, metric<span class="op">=</span><span class="st">'manhattan'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    model_manhattan.fit(X_tfidf, y_train_enc)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    y_predict_manhattan <span class="op">=</span> model_manhattan.predict(X_test_tfidf)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    f1_score_manhattan.append(f1_score(y_test_enc, y_predict_manhattan, average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="co">#plot the f1 scores for different k for both distance metric.</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, f1_score_list)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>plt.plot(ks, f1_score_manhattan)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value of k (Euclidean (Blue)), (Manhattan (Orange))'</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'F1 score'</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>best_k <span class="op">=</span> <span class="dv">11</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="co">#generate surface plot for selected k and distance metric</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>best_k, metric<span class="op">=</span><span class="st">'euclidean'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>model.fit(X_train2, y_train_enc)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X_train2[:,<span class="dv">0</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X_train2[:,<span class="dv">0</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X_train2[:,<span class="dv">1</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X_train2[:,<span class="dv">1</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min,x_max,<span class="dv">200</span>),</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>                    np.linspace(y_min,y_max,<span class="dv">200</span>))</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model.predict(grid).reshape(xx.shape)</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, levels<span class="op">=</span>[<span class="op">-</span><span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">1.5</span>], cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train2[:,<span class="dv">0</span>], X_train2[:,<span class="dv">1</span>], c<span class="op">=</span>y_train_enc, cmap<span class="op">=</span><span class="st">'coolwarm'</span>,</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>            edgecolor<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Component 1'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Component 2'</span>)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN Decision Surface (k=11, Euclidean)'</span>)</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a><span class="co"># shows same surface plot for selected k but for manhattan distance metric for comparison</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>model_manhattan <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">11</span>, metric<span class="op">=</span><span class="st">'manhattan'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>model_manhattan.fit(X_train2, y_train_enc)</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X_train2[:,<span class="dv">0</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X_train2[:,<span class="dv">0</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X_train2[:,<span class="dv">1</span>].<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X_train2[:,<span class="dv">1</span>].<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min,x_max,<span class="dv">200</span>),</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>                    np.linspace(y_min,y_max,<span class="dv">200</span>))</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> model_manhattan.predict(grid).reshape(xx.shape)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, levels<span class="op">=</span>[<span class="op">-</span><span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">1.5</span>], cmap<span class="op">=</span><span class="st">'coolwarm'</span>)</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train2[:,<span class="dv">0</span>], X_train2[:,<span class="dv">1</span>], c<span class="op">=</span>y_train_enc, cmap<span class="op">=</span><span class="st">'coolwarm'</span>,</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>            edgecolor<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Component 1'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'Component 2'</span>)</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN Decision Surface (k=11, Manhattan)'</span>)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-6-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the Euclidian example above, the F1 score reaches its peak at k = 10 where the F1 score is 0.9811… As K increases the F1 score steadily declines until pass roughly 400, where the fall off is significantly larger than the rest of the graph.</p>
<p>In the Manhattan example, the peak of our F1 score is at k = 1, or when there is only 1 neighbour at 0.7887… The data then springs erratically until it hits roughly 0.42 where it maintains the same F1 score until roughly 410~ neighbours when it suddenly drops, like the Euclidian example.</p>
<p>The boundary between the two classes is relatively smooth and nonlinear (with k=11 in this case), conforming to the distribution of points in the projected space. The chosen k (number of neighbours) influences the shape of this boundary: - Smaller k would make the boundary more jagged and sensitive to noise, possibly resulting in overfitting. - Larger k would produce a smoother boundary, potentially underfitting if too large (as local class details are averaged out).</p>
<p>The distance metric (Euclidean or Manhattan) determines how ‘closeness’ is calculated in the feature space, affecting which training samples count as neighbours for each test sample.</p>
<p>Euclidian: When the k is small, the F1 score is high, indicating that the classifier is extremely accurate. As the k increases, it smoothes out meaning that there is less overfitting until it drops sharply at around 400~ (underfitting).</p>
<p>Manhattan: When k is exactly 1, the F1 score reaches the peak of 0.7887, then the F1 score stays low as the k increases, indicating that the classifier is poorly performing.</p>
<p>The data has a very high dimension which needs to be reduced to plot our KNN. For this case we decided to use Singular Value Decomposition (SVD). This is particularly effective for TF-IDF tasks like the one we have.</p>
<p>This reduced the number of components but kept classification performance F1 score high while we decreased the training time, memory usage.</p>
<p>Surface Plot Euclidian Distance when k = 11 as our decision boundary. The difference curve of the two sides represents the data well as the more curve the graph has the better fit the classifier is for the data.</p>
</section>
<section id="support-vector-machines-svm" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines-svm">2.3 Support Vector Machines (SVM)</h2>
<p>Support Vector Machines (SVM) are supervised learning models that find the optimal separating hyperplane between classes by maximizing the margin—the distance between the hyperplane and the nearest data points (support vectors). For linearly separable data, the hard-margin SVM solves:</p>
<p><span class="math display">\[ \min_{\mathbf{w}, b} \frac{1}{2} |\mathbf{w}|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \]</span></p>
<p>For non-linearly separable data, soft-margin SVM introduces slack variables <span class="math inline">\(\xi_i\)</span> and a penalty parameter <span class="math inline">\(C\)</span> to allow misclassifications:</p>
<p><span class="math display">\[ \min_{\mathbf{w}, b, \xi} \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^n \xi_i \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \]</span></p>
<p>Kernelized SVMs use a kernel function <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j)\)</span> to implicitly map data into a higher-dimensional space, enabling non-linear decision boundaries. The dual optimization problem becomes:</p>
<p><span class="math display">\[ \max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \]</span></p>
<p>where <span class="math inline">\(\alpha_i\)</span> are Lagrange multipliers. Common kernels include polynomial and radial basis function (RBF). SVMs are robust to high-dimensional data and can be extended to multiclass problems using one-vs-rest or one-vs-one strategies.</p>
<div id="5c69bcb0" class="cell" data-execution_count="43">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the target variable categories from the DataFrame</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'Category'</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the sparse TF-IDF matrix to a dense array</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>X_dense <span class="op">=</span> X_tfidf.toarray()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Map categories to numeric codes for SVM compatibility</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>category_mapper <span class="op">=</span> pd.Categorical(y)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>y_numeric <span class="op">=</span> category_mapper.codes  <span class="co"># Numeric encoding of categories</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>category_names <span class="op">=</span> df[<span class="st">'Category'</span>]  <span class="co"># Original category names</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardise the features to have zero mean and unit variance</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X_dense)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA to reduce dimensionality to 2 components for visualisation</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X_scaled)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the penalty parameter for the SVM</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>C_penalty <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a linear SVM on the PCA-transformed data</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>linear_svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span>C_penalty)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>linear_svm.fit(X_pca, y)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the gamma parameter for the RBF kernel</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Train an RBF kernel SVM on the PCA-transformed data</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>rbf_svm <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>, gamma<span class="op">=</span>gamma)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>rbf_svm.fit(X_pca, y)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to plot the decision boundary of an SVM model</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_svc(model, X, y):</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the data points for each category</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    unique_categories <span class="op">=</span> np.unique(y)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> category <span class="kw">in</span> unique_categories:</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        plt.scatter(</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>            X[y <span class="op">==</span> category, <span class="dv">0</span>], X[y <span class="op">==</span> category, <span class="dv">1</span>],  <span class="co"># Select points for the current category</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span>category_names[category],  <span class="co"># Use category names for the legend</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>            s<span class="op">=</span><span class="dv">30</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>  <span class="co"># Set marker size and edge color</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the current axis limits</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> plt.gca()</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>    xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a grid to evaluate the decision function</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">500</span>),</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>                         np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">500</span>))</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.decision_function(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the decision boundary and margins</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    plt.contour(xx, yy, Z, levels<span class="op">=</span>[<span class="dv">0</span>], linewidths<span class="op">=</span><span class="dv">2</span>, linestyles<span class="op">=</span><span class="st">'solid'</span>, colors<span class="op">=</span><span class="st">'k'</span>)  <span class="co"># Decision boundary</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>    plt.contour(xx, yy, Z, levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], linewidths<span class="op">=</span><span class="dv">1</span>, linestyles<span class="op">=</span><span class="st">'dashed'</span>, colors<span class="op">=</span><span class="st">'k'</span>)  <span class="co"># Margins</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adjust the plot limits to avoid clipping</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> np.percentile(X[:, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">99</span>])</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> np.percentile(X[:, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">99</span>])</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>    plt.xlim(x_min <span class="op">-</span> <span class="dv">1</span>, x_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    plt.ylim(y_min <span class="op">-</span> <span class="dv">1</span>, y_max <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add labels, title and legend</span></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Principal Component 1"</span>)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Principal Component 2"</span>)</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"SVM Decision Boundary (kernel='</span><span class="sc">{</span>model<span class="sc">.</span>kernel<span class="sc">}</span><span class="ss">')"</span>)</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary for the linear SVM</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>plot_svc(linear_svm, X_pca, y_numeric)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary for the RBF kernel SVM</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>plot_svc(rbf_svm, X_pca, y_numeric)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The first plot above shows the decision boundary (solid black line) and margins (dashed lines) for a linear SVM with a penalty parameter C = 1. The SVM attempts to find a straight line (hyperplane in higher dimensions) that separates the two classes (“tech” and “entertainment”) while maximizing the margin and allowing some misclassifications due to the soft margin.</p>
<p>The penalty C on soft-margin decision boundaries controls the tradeoff between training error and maximising margin.</p>
<p>Small C</p>
<ul>
<li>Lower penalty on misclassified training points resulting in a wider margin</li>
<li>Less sensitive to outliers and noisy data points</li>
<li>Risk of underfitting as the model is too simple and general</li>
</ul>
<p>Large C</p>
<ul>
<li>Greater penalty on misclassified training points resulting in a tighter margin</li>
<li>More sensitive to outliers and noisy data points</li>
<li>Risk of overfitting as the model is to specific on training data</li>
</ul>
<p>The second plot above shows the non-linear decision boundaries for an SVM with an RBF kernel (gamma=1), again with C=1. Unlike the linear SVM, the RBF kernel can learn complex, curved boundaries that more flexibly adapt to the data distribution.</p>
<p>Small gamma</p>
<ul>
<li>Each training example has a large area of influence</li>
<li>Decision boundary is smoother, more generalised and less complex</li>
<li>Model aims for broad patterns potentially leading to underfitting</li>
</ul>
<p>Large gamma</p>
<ul>
<li>Each training example has a very localised area of influence</li>
<li>Decision boundary is highly complex and irregular</li>
<li>Model focuses on local patterns, very prone to overfitting the training data</li>
</ul>
<p>Visualization note: For interpretability, these plots use only the top two PCA components to project the high-dimensional TF-IDF data into two dimensions. We chose PCA because it captures the directions of greatest variance in the data, making the 2D visualization as informative as possible. However, the orange and blue data points are overlapping each other so perhaps a different visualization technique would be more effective. Actual model evaluation and hyperparameter tuning (in Q3) uses the full feature set.</p>
</section>
<section id="neural-networks-nn" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-nn">2.4 Neural Networks (NN)</h2>
<p>Artificial Neural Networks (ANNs) are computational models inspired by biological neurons, consisting of interconnected layers of units (neurons). Each neuron computes a weighted sum of its inputs, applies a nonlinear activation function <span class="math inline">\(f\)</span>, and outputs:</p>
<p><span class="math display">\[ y = f\left(\sum_{j} w_j x_j + b\right) \]</span></p>
<p>A single-layer perceptron can only model linearly separable data. Multi-layer feed-forward networks (MLPs) with at least one hidden layer and nonlinear activation functions (e.g., sigmoid, ReLU) can approximate any continuous function (Universal Approximation Theorem).</p>
<p>Training involves minimizing a loss function (e.g., mean squared error, cross-entropy) over the dataset using stochastic gradient descent (SGD) and backpropagation, which applies the chain rule to compute gradients for all weights:</p>
<p><span class="math display">\[ \mathbf{w} \leftarrow \mathbf{w} - \lambda \nabla_{\mathbf{w}} L(\mathbf{w}) \]</span></p>
<p>Regularization techniques (L1, L2, dropout, early stopping) help prevent overfitting by penalizing complex models or limiting training. Design choices include network depth, number of neurons, activation functions, learning rate, and initialization. Deep networks can suffer from vanishing/exploding gradients, which can be mitigated by using activation functions like ReLU and careful initialization.</p>
<div id="a4e40dd6" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load libraries</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MaxAbsScaler</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset (same as the others)</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform data into valid input to the nn-model. </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer.fit(train[<span class="st">'Text'</span>])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>le.fit(train[<span class="st">'Category'</span>])</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> tfidf_vectorizer.transform(train[<span class="st">'Text'</span>])</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> le.transform(train[<span class="st">'Category'</span>])</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">#print(x_train.shape[0])</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">#print(y_train.shape[0])</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> tfidf_vectorizer.transform(test[<span class="st">'Text'</span>])</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> le.transform(test[<span class="st">'Category'</span>])</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">#print(x_test.shape[0])</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">#print(y_test.shape[0])</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MaxAbsScaler()</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>scaler.fit(x_train)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="co">#Hide warnings </span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.exceptions <span class="im">import</span> ConvergenceWarning</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, category<span class="op">=</span>ConvergenceWarning)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="862e88fd" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to perform k-fold cross-validation on the training data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_fold_data(train, k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(train)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        fold_size <span class="op">=</span> n <span class="op">//</span> k</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        folds <span class="op">=</span> []</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            start <span class="op">=</span> i <span class="op">*</span> fold_size</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            end <span class="op">=</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> fold_size <span class="cf">if</span> i <span class="op">&lt;</span> k <span class="op">-</span> <span class="dv">1</span> <span class="cf">else</span> n  <span class="co"># handle last fold</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            val_set <span class="op">=</span> train.iloc[start:end]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>            train_set <span class="op">=</span> pd.concat([train.iloc[:start], train.iloc[end:]])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            folds.append((train_set, val_set))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> folds</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>five_fold <span class="op">=</span> k_fold_data(train, <span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="daae9d41" class="cell" data-execution_count="46">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score <span class="co"># may be useful in running NN.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="347b7012" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Function that returns a model for which you can predict upon input of testing datas. </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nn(x_train, y_train, unit_size): </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    x_train <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(unit_size,), activation<span class="op">=</span><span class="st">'relu'</span>, solver<span class="op">=</span><span class="st">'adam'</span>, max_iter<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>, warm_start<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    clf.fit(x_train, y_train) <span class="co">#initialise weight here</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.RandomState(<span class="dv">1</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    clf.coefs_[<span class="dv">0</span>] <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>clf.coefs_[<span class="dv">0</span>].shape) <span class="co">#deliberately initialise weight again from 0 to 0.1</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    clf.max_iter <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    clf.fit(x_train, y_train)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clf</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate the average entropy loss for training</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> average_entropy_train_loss(y_pred, y_true):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y_true)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> y_true[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> <span class="op">-</span>np.log(y_pred[i][<span class="dv">0</span>])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> <span class="op">-</span>np.log(y_pred[i][<span class="dv">1</span>])</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss<span class="op">/</span>n</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="669a5a48" class="cell" data-execution_count="48">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the neural network with different number of units and plot the average cross entropy loss (ACEL) for each</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>clf5 <span class="op">=</span> nn(x_train, y_train, <span class="dv">5</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>clf20 <span class="op">=</span> nn(x_train, y_train, <span class="dv">20</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>clf40 <span class="op">=</span> nn(x_train, y_train, <span class="dv">40</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the average cross entropy loss for each model</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>pred5 <span class="op">=</span> clf5.predict_proba(x_train)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>pred20 <span class="op">=</span> clf20.predict_proba(x_train)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>pred40 <span class="op">=</span> clf40.predict_proba(x_train)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the average cross entropy loss (ACEL) for each number of units</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>number_of_units <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">20</span>, <span class="dv">40</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>ACELs <span class="op">=</span> [average_entropy_train_loss(pred5, y_train), average_entropy_train_loss(pred20, y_train), average_entropy_train_loss(pred40, y_train)]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.plot(number_of_units, ACELs, marker<span class="op">=</span><span class="st">'o'</span>)  <span class="co"># 'marker' adds dots at each point</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of units'</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'ACEL (Avearge cross entropy loss)'</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Number of units against ACEL'</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on training data</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>pred5 <span class="op">=</span> clf5.predict(x_train)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>pred20 <span class="op">=</span> clf20.predict(x_train)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>pred40 <span class="op">=</span> clf40.predict(x_train)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Prediction accuracy: "</span>, accuracy_score(y_train, pred5), accuracy_score(y_train, pred20), accuracy_score(y_train, pred40))</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Prediction accuracy:  0.4953271028037383 0.9929906542056075 1.0
</code></pre>
</div>
</div>
<p>As the number of hidden units in the neural network increases, the average cross-entropy loss (ACEL) on the training data steadily decreases, showing that the model is learning to fit the data more closely. The largest improvement in ACEL occurs when increasing the hidden units from 5 to 20, while the reduction from 20 to 40 units is much smaller. This diminishing return suggests the model has reached its capacity to capture patterns in the data, and any remaining error is likely due to irreducible bias—limitations in the data or the model’s structure that cannot be overcome by simply adding more units. The training accuracy supports this, reaching 99% with 20 units. Increasing the number of units further may lead to overfitting, where the model memorizes the training data instead of generalizing to new, unseen examples. Thus, optimal performance is likely achieved with a moderate number of hidden units, balancing fit and generalization.</p>
</section>
</section>
<section id="model-comparison-and-hyperparameter-analysis" class="level1">
<h1>3. Model Comparison and Hyperparameter Analysis</h1>
<section id="training-set-size-vs.-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="training-set-size-vs.-accuracy">3.1 Training Set Size vs.&nbsp;Accuracy</h2>
<p><em>This section investigates how increasing the fraction of training data affects model performance. For each algorithm, models are trained on progressively larger subsets (10%, 30%, 50%, 70%, 90%) and their F1 scores on train and test sets are plotted to show learning curves and generalization.</em></p>
<section id="naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes">3.1.1 Naive Bayes</h3>
<div id="b5c7957e" class="cell" data-execution_count="49">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NB</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># F1 scores stored here for plotting</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>train_f1_scores <span class="op">=</span> []</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>test_f1_scores <span class="op">=</span> []</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Reload data</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare features and labels</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>X_train_full <span class="op">=</span> train_df[<span class="st">'Text'</span>].values</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>y_train_full <span class="op">=</span> train_df[<span class="st">'Category'</span>].values</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_df[<span class="st">'Text'</span>].values</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_df[<span class="st">'Category'</span>].values</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the vectorizer</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform the entire training data to get the vocabulary</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>X_train_full_transformed <span class="op">=</span> vectorizer.fit_transform(X_train_full)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>X_test_transformed <span class="op">=</span> vectorizer.transform(X_test)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Total number of samples in training set</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="bu">len</span>(X_train_full)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the fractions of data to use for training</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>fractions <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom F1 calculation</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_f1(precision, recall):</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> ((precision <span class="op">*</span> recall) <span class="op">/</span> (precision <span class="op">+</span> recall))</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Main loop</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> fractions:</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate how many samples to use for training</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    train_size <span class="op">=</span> <span class="bu">int</span>(m <span class="op">*</span> N)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use first m*N samples for training</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    X_train_subset <span class="op">=</span> X_train_full_transformed[:train_size]</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>    y_train_subset <span class="op">=</span> y_train_full[:train_size]</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the classifier</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    classifier <span class="op">=</span> MultinomialNB()</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    classifier.fit(X_train_subset, y_train_subset)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate training F1 on the training subset (the data it was trained on)</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    train_pred <span class="op">=</span> classifier.predict(X_train_subset)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>    train_precision <span class="op">=</span> precision_score(y_train_subset, train_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>    train_recall <span class="op">=</span> recall_score(y_train_subset, train_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>    train_f1 <span class="op">=</span> calculate_f1(train_precision, train_recall)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate testing F1 on the full test set</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    test_pred <span class="op">=</span> classifier.predict(X_test_transformed)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    test_precision <span class="op">=</span> precision_score(y_test, test_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>    test_recall <span class="op">=</span> recall_score(y_test, test_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>    test_f1 <span class="op">=</span> calculate_f1(test_precision, test_recall)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save F1 scores for plotting</span></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>    train_f1_scores.append(train_f1)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>    test_f1_scores.append(test_f1)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="co">#everything past here can be deleted, only used to show results in a plot</span></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store results for table</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>    results.append({</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Fraction'</span>: m,</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Training Size'</span>: train_size,</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Train Precision'</span>: train_precision,</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Train Recall'</span>: train_recall,</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Train F1'</span>: train_f1,</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Test Precision'</span>: test_precision,</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Test Recall'</span>: test_recall,</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Test F1'</span>: test_f1</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results table</span></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(results)</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Results:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results_df[[<span class="st">'Fraction'</span>, <span class="st">'Training Size'</span>, <span class="st">'Train F1'</span>, <span class="st">'Test F1'</span>]])</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Print raw F1 score lists (optional)</span></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training F1 Scores:"</span>, train_f1_scores)</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test F1 Scores:"</span>, test_f1_scores)</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>plt.plot([r[<span class="st">'Fraction'</span>] <span class="cf">for</span> r <span class="kw">in</span> results], [r[<span class="st">'Train F1'</span>] <span class="cf">for</span> r <span class="kw">in</span> results], marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Training F1'</span>)</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>plt.plot([r[<span class="st">'Fraction'</span>] <span class="cf">for</span> r <span class="kw">in</span> results], [r[<span class="st">'Test F1'</span>] <span class="cf">for</span> r <span class="kw">in</span> results], marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'Test F1'</span>)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Fraction of Training Data Used'</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'F1 Score'</span>)</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Effect of Training Data Size on F1 Score (Naive Bayes)'</span>)</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Classification Results:

   Fraction  Training Size  Train F1   Test F1
0       0.1             42  1.000000  0.559822
1       0.3            128  0.992246  0.981533
2       0.5            214  0.995349  0.963802
3       0.7            299  0.996667  0.963802
4       0.9            385  0.994805  0.972582

Training F1 Scores: [1.0, 0.9922457988130913, 0.995349354197689, 0.9966671310426931, 0.9948051948051949]
Test F1 Scores: [0.5598224362778794, 0.981533356484161, 0.9638019282521352, 0.9638019282521352, 0.9725817428952171]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>The training F1 remains extremely high (close to 1.0) across all training sizes, even at just 10% of the data. This suggests Naive Bayes is able to perfectly or near-perfectly fit the training data, consistent with the fact that NB is a simple, low-variance model and can memorize small datasets.</p></li>
<li><p>The test F1 score is much lower when the model is trained with only 10% of the data (≈0.56), reflecting poor generalization due to underfitting and limited information. As the training set fraction increases, the test F1 jumps sharply to ≈0.98 (at 30%), and then remains high (0.96–0.97) as more data is included.</p></li>
</ul>
<p>Generally, with very little data, NB cannot learn the full diversity of features, so generalization to unseen examples is poor. Increasing the training set size gives NB access to a richer vocabulary and better, more reliable probability estimates. As a result, model performance on the test set rapidly improves then plateaus. Past 30%, adding more data does little to improve generalization; the model’s test F1 stabilizes, indicating most useful patterns are already captured with moderate sample sizes.</p>
</section>
<section id="knn" class="level3">
<h3 class="anchored" data-anchor-id="knn">3.1.2 kNN</h3>
<div id="7d9459b6" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X_tfidf.shape[<span class="dv">0</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>m_list <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>train_f1 <span class="op">=</span> []</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>test_f1  <span class="op">=</span> []</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>best_k <span class="op">=</span> <span class="dv">11</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># iteratively split the training data into different fraction and passed to knn_f1 to find training and testing accuracy in F1.</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># do it for both </span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> m_list:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> <span class="bu">int</span>(m <span class="op">*</span> N)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    X_sub <span class="op">=</span> X_tfidf[:end]</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    y_sub <span class="op">=</span> y_train_enc[:end]</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    train_f1.append(</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        knn_f1(X_sub, y_sub, X_sub, y_sub, best_k)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    test_f1.append(</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        knn_f1(X_sub, y_sub, X_test_tfidf, y_test_enc, best_k)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co"># show the plot</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.plot(m_list, train_f1, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Train F1'</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.plot(m_list, test_f1,  marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Test F1'</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'m (fraction of training data)'</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Weighted F1 score'</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'KNN (k=</span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">) Learning Curve'</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We varied the proportion of training data (m) and measured train/test F1 scores for kNN with k=11 (Euclidean). The results show:</p>
<p>On the x axis, we have 0.1 to 0.9 with intervals of data given to us [0.1, 0.3, 0.5, 0.7, 0.9]. In our plotted graph, we see the weighted f1 score increase drastically from 0.1 to 0.3. Especially in the training data, this simply means that there is not enough data to form a nicely fitted classifier. Both the training and testing data plateaus at 0.3 to 0.9 with no increase in testing until 0.9, and minimal increase in training every fraction increase. Testing data is at all points higher than training, which can happen due to overfitting to small sets of data or by chance.</p>
<p>This confirms kNN needs a representative amount of data to reach peak performance; more data beyond a threshold doesn’t further improve generalization significantly.</p>
</section>
<section id="svm" class="level3">
<h3 class="anchored" data-anchor-id="svm">3.1.3 SVM</h3>
<div id="76748705" class="cell" data-execution_count="51">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>m_values <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X_pca.shape[<span class="dv">0</span>]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>train_f1_scores <span class="op">=</span> []</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>test_f1_scores <span class="op">=</span> []</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over different fractions of the training data</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> m_values:</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    mN <span class="op">=</span> <span class="bu">int</span>(m <span class="op">*</span> N)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    X_subset <span class="op">=</span> X_pca[:mN]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    y_subset <span class="op">=</span> y_numeric[:mN]</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    clf.fit(X_subset, y_subset)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training F1</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    y_subset_pred <span class="op">=</span> clf.predict(X_subset)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    train_f1_scores.append(f1_score(y_subset, y_subset_pred, average<span class="op">=</span><span class="st">'macro'</span>))</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Testing F1 (for demo, uses all data as "test" -- in real, use separate test set)</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    y_test_pred <span class="op">=</span> clf.predict(X_pca)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    test_f1_scores.append(f1_score(y_numeric, y_test_pred, average<span class="op">=</span><span class="st">'macro'</span>))</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"m=</span><span class="sc">{</span>m<span class="sc">:.1f}</span><span class="ss">: Train F1=</span><span class="sc">{</span>train_f1_scores[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">, Test F1=</span><span class="sc">{</span>test_f1_scores[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>plt.plot(m_values, train_f1_scores, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Train F1'</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>plt.plot(m_values, test_f1_scores, marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'Test F1'</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Fraction of Training Data'</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'F1 Score (macro)'</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'SVM Linear Learning Curve'</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>m=0.1: Train F1=0.373, Test F1=0.335
m=0.3: Train F1=0.655, Test F1=0.663
m=0.5: Train F1=0.696, Test F1=0.701
m=0.7: Train F1=0.719, Test F1=0.702
m=0.9: Train F1=0.712, Test F1=0.701</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As shown in the plot and table, both the training and testing macro F1 scores for a linear SVM increase rapidly as we add more training data, especially between 10% and 30% of the dataset. At very small training sizes (m = 0.1), the model underfits, achieving low F1 on both train and test. From m = 0.3 onwards, both scores increase and plateau in the 0.67–0.73 range. The test and train F1 scores are very close at all values of m, and the test F1 even slightly exceeds train F1 at some points—this can happen due to the random distribution of examples or when the test set happens to be slightly easier. Overall, increasing data beyond 50% of the training set brings diminishing returns for SVM, with little change in generalization performance.</p>
</section>
<section id="nn" class="level3">
<h3 class="anchored" data-anchor-id="nn">3.1.4 NN</h3>
<div id="7ccf8a30" class="cell" data-execution_count="52">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train NN on a fraction m of the training data</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mportion_nn(x_train, y_train, unit_size, m):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">int</span>(x_train.shape[<span class="dv">0</span>] <span class="op">*</span> m) <span class="co"># Number of samples to use</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    x_train <span class="op">=</span> x_train[:n]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> y_train[:n]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> MaxAbsScaler() <span class="co"># scale features for NN </span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    scaler.fit(x_train)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y_train</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create NN w one hidden layer of given unit size</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(unit_size,), activation<span class="op">=</span><span class="st">'relu'</span>, solver<span class="op">=</span><span class="st">'adam'</span>, max_iter<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>, warm_start<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    clf.fit(x, y) <span class="co">#initialise weight here</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.RandomState(<span class="dv">1</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    clf.coefs_[<span class="dv">0</span>] <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>clf.coefs_[<span class="dv">0</span>].shape) <span class="co">#deliberately initialise weight again from 0 to 0.1</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    clf.max_iter<span class="op">=</span><span class="dv">100</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    clf.fit(x, y)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clf, scaler</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">#Assume a unit of 20, we can compute the train_accuracy upon different size of training set:</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>clf10, scaler10 <span class="op">=</span> mportion_nn(x_train, y_train, <span class="dv">20</span>, <span class="fl">0.1</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>clf30, scaler30 <span class="op">=</span> mportion_nn(x_train, y_train, <span class="dv">20</span>, <span class="fl">0.3</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>clf50, scaler50 <span class="op">=</span> mportion_nn(x_train, y_train, <span class="dv">20</span>, <span class="fl">0.5</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>clf70, scaler70 <span class="op">=</span> mportion_nn(x_train, y_train, <span class="dv">20</span>, <span class="fl">0.7</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>clf90, scaler90 <span class="op">=</span> mportion_nn(x_train, y_train, <span class="dv">20</span>, <span class="fl">0.9</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform train/test data using each scaler </span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>input10train, input10test <span class="op">=</span> scaler10.transform(x_train), scaler10.transform(x_test)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>input30train, input30test <span class="op">=</span> scaler30.transform(x_train), scaler30.transform(x_test)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>input50train, input50test <span class="op">=</span> scaler50.transform(x_train), scaler50.transform(x_test)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>input70train, input70test <span class="op">=</span> scaler70.transform(x_train), scaler70.transform(x_test)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>input90train, input90test <span class="op">=</span> scaler90.transform(x_train), scaler90.transform(x_test)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">"10 percent f1(train):"</span>,</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(train[<span class="st">"Category"</span>]), clf10.predict(input10train), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"f1(test):"</span>,</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(test[<span class="st">"Category"</span>]),  clf10.predict(input10test),  average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="ch">\n</span><span class="st">30 percent f1(train):"</span>,</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(train[<span class="st">"Category"</span>]), clf30.predict(input30train), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">"f1(test):"</span>,</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(test[<span class="st">"Category"</span>]),  clf30.predict(input30test),  average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="ch">\n</span><span class="st">50 percent f1(train):"</span>,</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(train[<span class="st">"Category"</span>]), clf50.predict(input50train), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">"f1(test):"</span>,</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(test[<span class="st">"Category"</span>]),  clf50.predict(input50test),  average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="ch">\n</span><span class="st">70 percent f1(train):"</span>,</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(train[<span class="st">"Category"</span>]), clf70.predict(input70train), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"f1(test):"</span>,</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(test[<span class="st">"Category"</span>]),  clf70.predict(input70test),  average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="ch">\n</span><span class="st">90 percent f1(train):"</span>,</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(train[<span class="st">"Category"</span>]), clf90.predict(input90train), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"f1(test):"</span>,</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>    f1_score(le.transform(test[<span class="st">"Category"</span>]),  clf90.predict(input90test), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>10 percent f1(train): 0.6857441600323583 f1(test): 0.6951967415089199 
30 percent f1(train): 0.9296066095485795 f1(test): 0.8873989218328842 
50 percent f1(train): 0.9672553871356159 f1(test): 0.9529964146340217 
70 percent f1(train): 0.9883164174123119 f1(test): 0.9624274506868795 
90 percent f1(train): 1.0 f1(test): 0.9811799289034726</code></pre>
</div>
</div>
<div id="55e639d4" class="cell" data-execution_count="53">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Just hardcode the values from above output for plotting</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>f1train <span class="op">=</span> [<span class="fl">0.6857441600323583</span> , <span class="fl">0.9296066095485795</span> ,<span class="fl">0.9672553871356159</span> ,<span class="fl">0.9883164174123119</span> , <span class="fl">1.0</span>]</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>f1test <span class="op">=</span> [<span class="fl">0.6951967415089199</span> , <span class="fl">0.8873989218328842</span> , <span class="fl">0.9529964146340217</span>, <span class="fl">0.9624274506868795</span> , <span class="fl">0.9811799289034726</span>]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>percentage <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>plt.plot(percentage, f1train, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'F1 Train'</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>plt.plot(percentage, f1test, marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'F1 Test'</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Labels and title</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Percentage of Data Used for Training"</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1 Score"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"F1 Score vs Training Data Percentage"</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.7</span>, <span class="fl">1.05</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Finally, for the NN, we see a similar pattern where the graph shows that as the percentage of training data increases, both the training and testing F1 scores improve, indicating that the model benefits from having more data to learn from and hence to better generalization. At each percentage, the F1 score on the training set is consistently higher than on the test set, which is expected since the model is directly optimized on the training data.</p>
<p><strong>Conclusion</strong></p>
<p>The general trend is that all models show rapid improvement in test F1 as training size increases, plateauing after 30-50%. Furthermore, training F1 is always higher than (except in kNN) or close to test F1, indicating good generalization and minimal overfitting.</p>
</section>
</section>
<section id="hyperparameter-impact-via-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-impact-via-cross-validation">3.2 Hyperparameter Impact (via Cross-Validation)</h2>
<p><em>This section identifies the key hyperparameters for each model (e.g.&nbsp;alpha for Naive Bayes, k for kNN, C/gamma for SVM, units/activation for NN), and uses cross-validation to assess how changing these parameters affects performance. The results are compared and discussed to highlight which settings yield the best generalization.</em></p>
<section id="naive-bayes-1" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes-1">3.2.1 Naive Bayes</h3>
<div id="ab48a1ce" class="cell" data-execution_count="54">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare features and labels</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[<span class="st">'Text'</span>].values</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">'Category'</span>].values</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the TF-IDF vectorizer</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>X_train_tfidf <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define alpha values to investigate (Laplace smoothing parameter)</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>alpha_values <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">5.0</span>, <span class="fl">10.0</span>]</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom F1 calculation</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_f1(precision, recall):</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> ((precision <span class="op">*</span> recall) <span class="op">/</span> (precision <span class="op">+</span> recall))</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up 5-fold cross-validation</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>cv_results <span class="op">=</span> []</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="co"># For each alpha value</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alpha_values:</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    fold_f1_scores <span class="op">=</span> []</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-validation</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> train_index, val_index <span class="kw">in</span> kf.split(X_train_tfidf):</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split data into training and validation sets</span></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>        X_train_fold <span class="op">=</span> X_train_tfidf[train_index]</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>        y_train_fold <span class="op">=</span> y_train[train_index]</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>        X_val_fold <span class="op">=</span> X_train_tfidf[val_index]</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>        y_val_fold <span class="op">=</span> y_train[val_index]</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train MultinomialNB with current alpha</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>        nb <span class="op">=</span> MultinomialNB(alpha<span class="op">=</span>alpha)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>        nb.fit(X_train_fold, y_train_fold)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict and evaluate on validation set</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        y_val_pred <span class="op">=</span> nb.predict(X_val_fold)</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        val_precision <span class="op">=</span> precision_score(y_val_fold, y_val_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        val_recall <span class="op">=</span> recall_score(y_val_fold, y_val_pred, average<span class="op">=</span><span class="st">'weighted'</span>) </span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        val_f1 <span class="op">=</span> calculate_f1(val_precision, val_recall)</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        fold_f1_scores.append(val_f1)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average F1 score across folds</span></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>    mean_cv_f1 <span class="op">=</span> np.mean(fold_f1_scores)</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    cv_results.append(mean_cv_f1)</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Alpha = </span><span class="sc">{</span>alpha<span class="sc">:.2f}</span><span class="ss">: CV F1 = </span><span class="sc">{</span>mean_cv_f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>plt.plot(alpha_values, cv_results, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'5-Fold CV F1 Score'</span>)</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)  <span class="co"># Log scale for alpha values</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Alpha (Laplace Smoothing Parameter)'</span>)</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'F1 Score'</span>)</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of Alpha Parameter on Naive Bayes Cross-Validation Performance'</span>)</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Alpha = 0.01: CV F1 = 0.9771
Alpha = 0.10: CV F1 = 0.9839
Alpha = 0.50: CV F1 = 0.9796
Alpha = 1.00: CV F1 = 0.9772
Alpha = 2.00: CV F1 = 0.9751
Alpha = 5.00: CV F1 = 0.9664
Alpha = 10.00: CV F1 = 0.9579</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here, we investigated the effect of the Laplace smoothing hyperparameter, alpha, on cross-validated classification performance using 5-fold cross-validation. Laplace smoothing helps handle features (words) that occur in the test set but are unseen in a class of the training set, by adding a small count (“pseudocount”) everywhere.</p>
<p>Performance trend: - As shown in the plot above, the F1 score is sensitive to the value of alpha. The cross-validated F1 peaks at alpha=0.1 (CV F1=0.984). So with low and moderate values of alpha, the classifier tends to generalize well to validation folds.</p>
<p>Small alpha (less smoothing =&gt; lower bias, higher variance): - At very low alpha (e.g.&nbsp;0.01), we see good generalization, but performance slightly lags the optimum. This is because with too little smoothing, probabilities for rare or unseen words are underestimated, making the model brittle to vocabulary variation.</p>
<p>Large alpha (more smoothing =&gt; higher bias, lower variance): - Beyond alpha = 2, the F1 score drops noticeably. This is because excessive smoothing ‘washes out’ the true signal in word frequencies: highly distinctive words lose importance relative to the class prior, and classification becomes less discriminative.</p>
<p>Hence, when alpha~0.1, it yields the best generalization, balancing noise reduction and feature distinctiveness.</p>
</section>
<section id="knn-1" class="level3">
<h3 class="anchored" data-anchor-id="knn-1">3.2.2 kNN</h3>
<p>We choose odd values of k from 3 to 75 for kNN to ensure there is always a clear majority class among the neighbors, avoiding ties in classification.</p>
<div id="cf9197b4" class="cell" data-execution_count="55">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">3</span>, <span class="dv">76</span>, <span class="dv">2</span>)) </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>f1_scorer <span class="op">=</span> make_scorer(f1_score, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># for each key hyperparameters, find mean f1 score on 5-fold cross-validation</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>records <span class="op">=</span> []</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> KNeighborsClassifier(</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        n_neighbors<span class="op">=</span>k,</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        metric<span class="op">=</span><span class="st">'euclidean'</span>,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_val_score(</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        clf,</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        X_train_tfidf,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        y_train_enc,</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        cv<span class="op">=</span>cv,</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        scoring<span class="op">=</span>f1_scorer,</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>    records.append({<span class="st">'k'</span>: k, <span class="st">'mean_f1'</span>: scores.mean()})</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>df_cv <span class="op">=</span> pd.DataFrame(records)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a><span class="co"># show the plot</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">4</span>))</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>plt.plot(df_cv[<span class="st">'k'</span>], df_cv[<span class="st">'mean_f1'</span>], marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'k (odd values)'</span>)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'5-fold weighted F1'</span>)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN (Euclidean): impact of k on CV-F1'</span>)</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="co"># output top 3 k that result in highest f1 score with 5-fold cross-validation and their f1 scores.</span></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>top3 <span class="op">=</span> df_cv.nlargest(<span class="dv">3</span>, <span class="st">'mean_f1'</span>)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOP 3 k-values by CV-F1:"</span>)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, row <span class="kw">in</span> top3.iterrows():</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"k = </span><span class="sc">{</span><span class="bu">int</span>(row.k)<span class="sc">}</span><span class="ss">, CV-F1 = </span><span class="sc">{</span>row<span class="sc">.</span>mean_f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute sqrt(N)</span></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> X_tfidf.shape[<span class="dv">0</span>]</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">N ="</span>, N)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sqrt(N) ="</span>, np.sqrt(N))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>TOP 3 k-values by CV-F1:
k = 43, CV-F1 = 0.9790
k = 45, CV-F1 = 0.9790
k = 7, CV-F1 = 0.9789

N = 428
sqrt(N) = 20.688160865577203</code></pre>
</div>
</div>
<p>We performed stratified 5-fold cross-validation on selected k values and report the mean weighted F1 for each:</p>
<ul>
<li>The top mean F1 occurs at k=7, k=43, and k=45 (mean F1 ≈ 0.979).</li>
<li>Both moderate and small odd k (e.g., 7, 43, 45) perform very well. As k increases above ~53, mean F1 drops, especially at very large k, due to increased underfitting. This aligns with kNN theory (optimal k is often near √N or slightly higher; too large = majority class).</li>
<li>The optimal k should be selected using cross-validation—while the model is robust for a range of odd k, bad choices can hurt accuracy.</li>
</ul>
<p>We will consider all three k values in the final evaluation.</p>
</section>
<section id="svm-1" class="level3">
<h3 class="anchored" data-anchor-id="svm-1">3.2.3 SVM</h3>
<div id="f474754e" class="cell" data-execution_count="56">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>C_values <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'linear'</span>: {<span class="st">'train_f1'</span>: [], <span class="st">'test_f1'</span>: [], <span class="st">'C'</span>: []},</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rbf'</span>:    {<span class="st">'train_f1'</span>: [], <span class="st">'test_f1'</span>: [], <span class="st">'C'</span>: []}</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kernel <span class="kw">in</span> [<span class="st">'linear'</span>, <span class="st">'rbf'</span>]:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running kernel: </span><span class="sc">{</span>kernel<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> C <span class="kw">in</span> C_values:</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train SVM with specified kernel and penalty parameter C</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        clf <span class="op">=</span> SVC(kernel<span class="op">=</span>kernel, C<span class="op">=</span>C)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        clf.fit(X_pca, y_numeric)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Evaluate F1 score on the same PCA-reduced training set</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        train_pred <span class="op">=</span> clf.predict(X_pca)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        train_f1 <span class="op">=</span> f1_score(y_numeric, train_pred, average<span class="op">=</span><span class="st">'macro'</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For this analysis, test F1 is set equal to train F1 (since no separate test set in PCA space)</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>        test_f1 <span class="op">=</span> train_f1  </span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        results[kernel][<span class="st">'C'</span>].append(C)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        results[kernel][<span class="st">'train_f1'</span>].append(train_f1)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>        results[kernel][<span class="st">'test_f1'</span>].append(test_f1)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Kernel=</span><span class="sc">{</span>kernel<span class="sc">}</span><span class="ss">, C=</span><span class="sc">{</span>C<span class="sc">}</span><span class="ss">: Train F1=</span><span class="sc">{</span>train_f1<span class="sc">:.3f}</span><span class="ss">, Test F1=</span><span class="sc">{</span>test_f1<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>plt.plot(results[<span class="st">'linear'</span>][<span class="st">'C'</span>], results[<span class="st">'linear'</span>][<span class="st">'test_f1'</span>], marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Linear'</span>)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>plt.plot(results[<span class="st">'rbf'</span>][<span class="st">'C'</span>], results[<span class="st">'rbf'</span>][<span class="st">'test_f1'</span>], marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'RBF'</span>)</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'C'</span>)</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Test F1 (macro)'</span>)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'SVM: Hyperparameter Impact'</span>)</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Running kernel: linear
Kernel=linear, C=1: Train F1=0.701, Test F1=0.701
Kernel=linear, C=2: Train F1=0.704, Test F1=0.704
Kernel=linear, C=3: Train F1=0.704, Test F1=0.704
Kernel=linear, C=4: Train F1=0.704, Test F1=0.704
Kernel=linear, C=5: Train F1=0.704, Test F1=0.704
Kernel=linear, C=6: Train F1=0.704, Test F1=0.704
Kernel=linear, C=7: Train F1=0.704, Test F1=0.704
Kernel=linear, C=8: Train F1=0.704, Test F1=0.704
Kernel=linear, C=9: Train F1=0.704, Test F1=0.704
Kernel=linear, C=10: Train F1=0.704, Test F1=0.704
Running kernel: rbf
Kernel=rbf, C=1: Train F1=0.606, Test F1=0.606
Kernel=rbf, C=2: Train F1=0.630, Test F1=0.630
Kernel=rbf, C=3: Train F1=0.657, Test F1=0.657
Kernel=rbf, C=4: Train F1=0.685, Test F1=0.685
Kernel=rbf, C=5: Train F1=0.681, Test F1=0.681
Kernel=rbf, C=6: Train F1=0.684, Test F1=0.684
Kernel=rbf, C=7: Train F1=0.688, Test F1=0.688
Kernel=rbf, C=8: Train F1=0.688, Test F1=0.688
Kernel=rbf, C=9: Train F1=0.693, Test F1=0.693
Kernel=rbf, C=10: Train F1=0.689, Test F1=0.689</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="BBC_News_Classification_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We evaluated the impact of the penalty parameter C for both linear and RBF SVMs. For the linear kernel, F1 scores are stable across all C values, indicating that the model is not sensitive to this parameter on this dataset (F1 ≈ 0.71). For the RBF kernel, increasing C gradually improves F1: starting from 0.60 (C=1) and rising to about 0.70 (C=10). However, RBF never surpasses the linear kernel in this case. This suggests that the linear decision boundary is already quite close to optimal for this dataset, likely due to limited non-linearity or the effect of using only two PCA components.</p>
<ul>
<li>Linear kernel: Nearly constant performance across C, best F1 ≈ 0.71</li>
<li>RBF kernel: F1 increases with C, but always slightly lower than the linear kernel, best F1 ≈ 0.70</li>
</ul>
<p>Both SVM variants struggle to perfectly separate the two classes with the projected features; the linear SVM performs best. Additionally, more informative features or further tuning (such as adjusting gamma for RBF, or using the full feature set rather than 2D PCA) might be required for improved separation.</p>
<p>Important note:</p>
<p>PCA-based visualization results are not comparable to full-feature results -&gt; See <em>Final Notes</em>.</p>
</section>
<section id="nn-1" class="level3">
<h3 class="anchored" data-anchor-id="nn-1">3.2.4 NN</h3>
<div id="2b4d4a34" class="cell" data-execution_count="57">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score <span class="co"># initial weight has to be between [0, 0.1], we don't use this for now</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 5-fold CV for NN with given unit size and activation function</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_validate(folds, unit_size, activation <span class="op">=</span> <span class="st">'relu'</span>):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(folds) </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for each fold, train on training set and evaluate on validation set</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        scaler <span class="op">=</span> MaxAbsScaler() <span class="co"># Create a new scaler for each fold</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit vectorizer and scaler on training fold</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        tfidf_vectorizer.fit(folds[i][<span class="dv">0</span>][<span class="st">'Text'</span>])</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        x_train <span class="op">=</span> tfidf_vectorizer.transform(folds[i][<span class="dv">0</span>][<span class="st">'Text'</span>])</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        scaler.fit(x_train)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>        x_train <span class="op">=</span> scaler.transform(x_train)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        y_train <span class="op">=</span> le.transform(folds[i][<span class="dv">0</span>][<span class="st">'Category'</span>])</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transform validation fold using fitted vectorizer and scaler</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        x_test <span class="op">=</span> tfidf_vectorizer.transform(folds[i][<span class="dv">1</span>][<span class="st">'Text'</span>])</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        x_test <span class="op">=</span> scaler.transform(x_test)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        y_test <span class="op">=</span> le.transform(folds[i][<span class="dv">1</span>][<span class="st">'Category'</span>])</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create NN w one hidden layer of given unit size and activation function</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(unit_size,), activation<span class="op">=</span>activation, solver<span class="op">=</span><span class="st">'adam'</span>, max_iter<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>, warm_start<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        clf.fit(x_train, y_train) <span class="co">#initialise weight here</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        rng <span class="op">=</span> np.random.RandomState(<span class="dv">1</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        clf.coefs_[<span class="dv">0</span>] <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>clf.coefs_[<span class="dv">0</span>].shape) <span class="co">#deliberately initialise weight again from 0 to 0.1, which is what cross_val_score() can't do</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>        clf.max_iter<span class="op">=</span><span class="dv">100</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>        clf.fit(x_train, y_train)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict on validation fold and accumulate accuracy</span></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> clf.predict(x_test)</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>        accuracy <span class="op">+=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy<span class="op">/</span>n <span class="co"># Return average accuracy across all folds</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for NN with units from 15 to 50 (logistic activation)</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> units <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>, <span class="dv">51</span>, <span class="dv">5</span>):  </span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>units<span class="sc">}</span><span class="ss"> units: (logistic)"</span>, cross_validate(five_fold, units, <span class="st">'logistic'</span>))</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for NN with units from 15 to 50 (relu activation)</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> units <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>, <span class="dv">51</span>, <span class="dv">5</span>): </span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>units<span class="sc">}</span><span class="ss"> units: (relu)"</span>, cross_validate(five_fold, units, <span class="st">'relu'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>15 units: (logistic) 0.7836096256684492
20 units: (logistic) 0.7610962566844919
25 units: (logistic) 0.8683155080213902
30 units: (logistic) 0.8669786096256684
35 units: (logistic) 0.9600802139037432
40 units: (logistic) 0.9600802139037432
45 units: (logistic) 0.9624331550802138
50 units: (logistic) 0.9600802139037432
15 units: (relu) 0.8612566844919787
20 units: (relu) 0.8575668449197862
25 units: (relu) 0.8565508021390373
30 units: (relu) 0.8599197860962567
35 units: (relu) 0.9624331550802138
40 units: (relu) 0.9483155080213903
45 units: (relu) 0.9579679144385025
50 units: (relu) 0.9507486631016041</code></pre>
</div>
</div>
<p>Parameters used in NN:</p>
<ul>
<li>number of layers (fixed to 1)</li>
<li>bias (not used here)</li>
<li><strong>number of units per layer (unit_size which is tunable)</strong></li>
<li><strong>activation Functions (tunable here, default is relu)</strong></li>
<li>learning rate (fixed to 0.01)</li>
<li>epochs (fix to 100)</li>
<li>optimizer (not explored here)</li>
</ul>
<p><strong>Logistic activation:</strong> Accuracy rises sharply from 0.76 (20 units) to 0.96 (35–40 units), showing much better generalization as units increase.</p>
<p><strong>ReLU activation:</strong> Accuracy is stable (0.85–0.86) for 20–30 units, then jumps to 0.96 at 35 units, but slightly drops at 40–50 units.</p>
<p>The best NN performance is seen with 35–45 units (both activations). The activation function does affect results: logistic performs better at higher unit counts, while relu is more stable at lower counts but peaks at 35 units. This suggests that for this dataset, increasing hidden units improves generalization up to a point, and logistic activation may be preferable for larger networks.</p>
</section>
</section>
<section id="fit-optimized-models" class="level2">
<h2 class="anchored" data-anchor-id="fit-optimized-models">3.3 Fit Optimized Models</h2>
<p><em>We retrain each model using the best hyperparameters found previously, evaluate them on the test set, and compare their final weighted F1 scores in section 3.4.</em></p>
<section id="retrain-nb" class="level3">
<h3 class="anchored" data-anchor-id="retrain-nb">3.3.1 Retrain NB</h3>
<div id="a23858f0" class="cell" data-execution_count="58">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> MultinomialNB</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> train_df[<span class="st">'Text'</span>]</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_df[<span class="st">'Category'</span>]</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_df[<span class="st">'Text'</span>]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_df[<span class="st">'Category'</span>]</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectorize text</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>X_train_tfidf <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>X_test_tfidf <span class="op">=</span> vectorizer.transform(X_test)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrain NB with best alpha</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> MultinomialNB(alpha<span class="op">=</span>best_alpha)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>nb.fit(X_train_tfidf, y_train)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and score F1 on test set</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> nb.predict(X_test_tfidf)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_test, y_pred_test, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Naive Bayes (alpha=</span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">) Test Weighted F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Naive Bayes (alpha=0.1) Test Weighted F1 Score: 0.9718</code></pre>
</div>
</div>
</section>
<section id="retrain-knn" class="level3">
<h3 class="anchored" data-anchor-id="retrain-knn">3.3.2 Retrain kNN</h3>
<div id="29271371" class="cell" data-execution_count="59">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>df_train <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the same preprocessing as before</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>X_train_tfidf <span class="op">=</span> tfidf_vectorizer.fit_transform(df_train[<span class="st">'Text'</span>])</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>X_test_tfidf <span class="op">=</span> tfidf_vectorizer.transform(df_test[<span class="st">'Text'</span>])</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode labels</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> {<span class="st">'entertainment'</span>: <span class="dv">0</span>, <span class="st">'tech'</span>: <span class="dv">1</span>}</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>y_train_enc <span class="op">=</span> df_train[<span class="st">'Category'</span>].<span class="bu">map</span>(encoder).values</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>y_test_enc  <span class="op">=</span> df_test[<span class="st">'Category'</span>].<span class="bu">map</span>(encoder).values</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrain kNN on full train set with best k found earlier (7, 43, or 45)</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>best_k <span class="op">=</span> <span class="dv">7</span> <span class="co"># 43, 45 gave Test Weighted F1 Score: 0.9717</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>best_k, metric<span class="op">=</span><span class="st">'euclidean'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train_tfidf, y_train_enc)</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict &amp; score</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> knn.predict(X_test_tfidf)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_test_enc, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"kNN (k=</span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">, Euclidean) Test Weighted F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>kNN (k=7, Euclidean) Test Weighted F1 Score: 0.9811</code></pre>
</div>
</div>
</section>
<section id="retrain-svm" class="level3">
<h3 class="anchored" data-anchor-id="retrain-svm">3.3.3 Retrain SVM</h3>
<div id="257181c0" class="cell" data-execution_count="60">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>df_train <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>df_test <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> df_train[<span class="st">'Text'</span>]</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> df_train[<span class="st">'Category'</span>]</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>X_test  <span class="op">=</span> df_test[<span class="st">'Text'</span>]</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>y_test  <span class="op">=</span> df_test[<span class="st">'Category'</span>]</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Vectorize: fit on train, transform both</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>X_train_tfidf <span class="op">=</span> vectorizer.fit_transform(X_train)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>X_test_tfidf  <span class="op">=</span> vectorizer.transform(X_test)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="co"># SVM is sensitive to scaling; use StandardScaler on dense versions</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler(with_mean<span class="op">=</span><span class="va">False</span>)  <span class="co"># for sparse input, set with_mean=False</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train_tfidf)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>X_test_scaled  <span class="op">=</span> scaler.transform(X_test_tfidf)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode labels numerically, in a consistent way for both</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>label_map <span class="op">=</span> {<span class="st">'entertainment'</span>: <span class="dv">0</span>, <span class="st">'tech'</span>: <span class="dv">1</span>}</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>y_train_enc <span class="op">=</span> y_train.<span class="bu">map</span>(label_map)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>y_test_enc  <span class="op">=</span> y_test.<span class="bu">map</span>(label_map)</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrain Linear SVM</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>clf_linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'linear'</span>, C<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>clf_linear.fit(X_train_scaled, y_train_enc)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>y_pred_linear <span class="op">=</span> clf_linear.predict(X_test_scaled)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>f1_linear <span class="op">=</span> f1_score(y_test_enc, y_pred_linear, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SVM (linear, C=1): Test Weighted F1 = </span><span class="sc">{</span>f1_linear<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrain RBF SVM (best C=10, but can test with other C/gamma)</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>clf_rbf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>, C<span class="op">=</span><span class="dv">10</span>, gamma<span class="op">=</span><span class="st">'scale'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>clf_rbf.fit(X_train_scaled, y_train_enc)</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>y_pred_rbf <span class="op">=</span> clf_rbf.predict(X_test_scaled)</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>f1_rbf <span class="op">=</span> f1_score(y_test_enc, y_pred_rbf, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SVM (rbf, C=10, gamma='scale'): Test Weighted F1 = </span><span class="sc">{</span>f1_rbf<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SVM (linear, C=1): Test Weighted F1 = 0.9812
SVM (rbf, C=10, gamma='scale'): Test Weighted F1 = 0.8873</code></pre>
</div>
</div>
</section>
<section id="retrain-nn" class="level3">
<h3 class="anchored" data-anchor-id="retrain-nn">3.3.4 Retrain NN</h3>
<div id="6b9ffe75" class="cell" data-execution_count="61">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic activation:</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>test  <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode and scale consistently</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> tfidf_vectorizer.fit_transform(train[<span class="st">'Text'</span>])</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>x_test  <span class="op">=</span> tfidf_vectorizer.transform(test[<span class="st">'Text'</span>])</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> le.fit_transform(train[<span class="st">'Category'</span>])</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>y_test  <span class="op">=</span> le.transform(test[<span class="st">'Category'</span>])</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MaxAbsScaler()</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> scaler.fit_transform(x_train)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>x_test  <span class="op">=</span> scaler.transform(x_test)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Define and fit NN with best params </span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>unit_size <span class="op">=</span> <span class="dv">45</span> </span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> <span class="st">'logistic'</span>  <span class="co"># or 'relu' </span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom model init to set weights in [0,0.1] (assignment spec)</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(unit_size,), activation<span class="op">=</span>activation, </span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>                    solver<span class="op">=</span><span class="st">'adam'</span>, max_iter<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>, warm_start<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>clf.fit(x_train, y_train)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">1</span>)</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>clf.coefs_[<span class="dv">0</span>] <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>clf.coefs_[<span class="dv">0</span>].shape)</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>clf.max_iter <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>clf.fit(x_train, y_train)</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on test and score</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(x_test)</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neural Network (units=</span><span class="sc">{</span>unit_size<span class="sc">}</span><span class="ss">, activation=</span><span class="sc">{</span>activation<span class="sc">}</span><span class="ss">) Test Weighted F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f"Neural Network (units={unit_size}, activation={activation}) Test Accuracy: {acc:.4f}")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network (units=45, activation=logistic) Test Weighted F1 Score: 0.9812</code></pre>
</div>
</div>
<div id="d221cf87" class="cell" data-execution_count="62">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># try relu activation</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">'train.csv'</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>test  <span class="op">=</span> pd.read_csv(<span class="st">'test.csv'</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode and scale consistently</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>tfidf_vectorizer <span class="op">=</span> TfidfVectorizer()</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> tfidf_vectorizer.fit_transform(train[<span class="st">'Text'</span>])</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>x_test  <span class="op">=</span> tfidf_vectorizer.transform(test[<span class="st">'Text'</span>])</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> le.fit_transform(train[<span class="st">'Category'</span>])</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>y_test  <span class="op">=</span> le.transform(test[<span class="st">'Category'</span>])</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MaxAbsScaler()</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> scaler.fit_transform(x_train)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>x_test  <span class="op">=</span> scaler.transform(x_test)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define and fit NN with best params </span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>unit_size <span class="op">=</span> <span class="dv">35</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>activation <span class="op">=</span> <span class="st">'relu'</span>  </span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom model init to set weights in [0,0.1] (assignment spec)</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(unit_size,), activation<span class="op">=</span>activation, </span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>                    solver<span class="op">=</span><span class="st">'adam'</span>, max_iter<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">1</span>, warm_start<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>clf.fit(x_train, y_train)</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">1</span>)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>clf.coefs_[<span class="dv">0</span>] <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>clf.coefs_[<span class="dv">0</span>].shape)</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>clf.max_iter <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>clf.fit(x_train, y_train)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on test and score</span></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(x_test)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neural Network (units=</span><span class="sc">{</span>unit_size<span class="sc">}</span><span class="ss">, activation=</span><span class="sc">{</span>activation<span class="sc">}</span><span class="ss">) Test Weighted F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f"Neural Network (units={unit_size}, activation={activation}) Test Accuracy: {acc:.4f}")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network (units=35, activation=relu) Test Weighted F1 Score: 0.9718</code></pre>
</div>
</div>
<p>The NN model with the logistic activation yields a higher test accuracy than that with the relu activation, so we will use that model for the final evaluation.</p>
</section>
</section>
<section id="final-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="final-evaluation">3.4 Final Evaluation</h2>
<p>Finally, we evaluated all four supervised learning models — Naive Bayes, kNN, SVM (with both linear and RBF kernels), and Neural Network — on the provided BBC news test set, using the best hyperparameters determined from previous cross-validation experiments. Weighted F1 score was used for fair comparison.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Rank</th>
<th>Model</th>
<th>Best Hyperparameters</th>
<th>Test Weighted F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Neural Network</td>
<td>1 hidden layer, units=45, activation=logistic</td>
<td>0.9812</td>
</tr>
<tr class="even">
<td>1</td>
<td>SVM (linear)</td>
<td>C=1</td>
<td>0.9812</td>
</tr>
<tr class="odd">
<td>3</td>
<td>kNN</td>
<td>k=7, Euclidean</td>
<td>0.9811</td>
</tr>
<tr class="even">
<td>4</td>
<td>Naive Bayes</td>
<td>alpha=0.1</td>
<td>0.9718</td>
</tr>
<tr class="odd">
<td>5</td>
<td>SVM (RBF)</td>
<td>C=10, gamma=‘scale’</td>
<td>0.8873</td>
</tr>
</tbody>
</table>
<p><strong>Observations</strong> - Neural Network, Linear SVM, and kNN all achieved nearly identical, outstanding performance, with weighted F1 scores above 0.98, indicating that the majority of test articles were correctly classified with only minor misclassifications. This demonstrates strong generalization when given high-dimensional TF-IDF features for this binary classification task. - Naive Bayes also performed very well, but was slightly behind the top ML models (F1 ~ 0.97). This is consistent with expectations: while NB is fast and efficient, its conditional independence assumption is less expressive for text than methods like SVM and NN. - SVM with the RBF kernel performed significantly worse (F1 ~ 0.89) compared to the linear kernel. This is likely because the high-dimensional TF-IDF representation makes the data close to linearly separable; non-linear kernels may therefore introduce unnecessary complexity and even degrade performance.</p>
<p>The extremely close F1 values of the top three models indicate that, at least for this dataset and feature space, most mainstream algorithms are capable of near-perfect article classification after hyperparameter tuning.</p>
<section id="final-notes" class="level3">
<h3 class="anchored" data-anchor-id="final-notes">Final Notes</h3>
<p>It is important to note that the relatively modest F1 scores (~0.71) observed in the hyperparameter validation of SVM in Q3b were based on models trained and evaluated only on the top two principal components (PCA) of the TF-IDF feature space. This 2D reduction was necessary to visualize SVM decision surfaces, but it also severely restricts the amount of information available to the classifier — making the classification task more difficult and limiting the model’s best achievable F1.</p>
<p>In contrast, the final evaluation in part 3.3 was performed on the full, high-dimensional TF-IDF feature set, where the SVM (linear kernel) can make use of thousands of features. The much higher final test F1 (~0.98) in this setting demonstrates that the true structure of the data is captured in the high-dimensional space, and linear separation is nearly optimal. Thus, the best-case performance of SVM on this task is only realized when all original features are included; using PCA solely for low-dimensional visualization understates the SVM’s actual capability.</p>
<section id="end-of-report" class="level4">
<h4 class="anchored" data-anchor-id="end-of-report"><em>End of Report</em></h4>
</section>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="BBC_News_Classification_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>